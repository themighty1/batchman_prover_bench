# Batchman Investigation

Code: https://github.com/themighty1/batchman_prover_bench

**TL;DR:** Under a 1 MB prover bandwidth constraint, the unmodified Batchman protocol achieves a proving rate of ~1 kHz. We describe optimizations which may increase the proving rate to ~20 kHz.

This research investigates the feasibility of deploying the Batchman protocol in a two-party computation (2PC) setting, with the goal of enabling a fast zero-knowledge virtual machine (ZKVM). Batchman is a zero-knowledge protocol that natively supports both arithmetic and Boolean circuits. As a first step, we evaluated the protocol in its unmodified form, benchmarking performance in both circuit settings. These initial benchmarks revealed that at a proving rate of 1 kHz, the protocol reaches the prover bandwidth limit, given a baseline constraint of 1 MB upload and 1 MB download. Motivated by this result, we explore whether specific sub-protocols within Batchman can be swapped out or optimized to reduce communication overhead and improve performance in a 2PC environment. Under this bandwidth constraint, we assess the overall feasibility of the protocol and identify potential avenues for making Batchman practical in this setting.

## Communication lower bound

There is a fundamental communication lower bound that no sub-protocol optimization can eliminate. It arises from VOLE correction: parties initially generate random VOLEs without knowing the witness, and must later transmit adjustments once the witness is known.

In the arithmetic setting (field size ~8 bits), each witness value requires an 8-byte correction. In the Boolean setting, this reduces to one bit per gate (a special case of OT). This correction cost dominates prover bandwidth and represents a hard lower bound on communication in Batchman-based 2PC. Overcoming it would require a fundamentally different approach such as homomorphic encryption, which is outside the scope of this work.

We assume VOLEs and OTs are generated by an external silent protocol with negligible communication overhead, so their generation cost is excluded from our bandwidth analysis.

That said, VOLE/OT generation does incur nontrivial computational cost (varying between arithmetic and Boolean settings). We treat this as an orthogonal concern and do not account for it in the performance evaluation.

Given these observations, the arithmetic circuit variant of the protocol is not viable for our target setting. The use of larger fields leads to a substantially higher communication cost—on the order of 64× compared to the binary case—which exceeds the available prover bandwidth even under optimistic assumptions. Consequently, the remainder of this work focuses exclusively on optimizing the binary version of the Batchman protocol.

## Optimization 1: Replacing LPZK with PCS

In Batchman, the disjunction proof must show that for each batch instance, at least one branch proof (an IT-MAC over GF(2^128)) is zero — corresponding to the branch actually executed. The original protocol uses LPZK: the prover computes a multiplication tree over all branch proofs and proves the product is zero. This requires sending (branch_sz - 1) intermediate wire commitments per batch to the verifier — O(branch_sz * batch_sz * 16 bytes). For realistic parameters (e.g. 1200 branches, 10K batches) this is ~192 MB of prover upload, far exceeding our 1 MB budget.

PCS mode replaces this with a three-step protocol:

1. The prover commits to the MAC values of its active (zero) branches across all batches using a polynomial commitment scheme.
2. The verifier reveals its OT delta, allowing the prover to reconstruct the MAC keys for every branch.
3. The prover proves that each committed value is a member of the corresponding set of verifier keys — i.e., a subset proof.

This eliminates the per-branch-per-batch communication entirely. The PCS proof is ~1 MB for the entire set of MACs regardless of branch count. The trade-off is prover computation: PCS requires NTT/FFT-heavy polynomial operations that take tens of seconds on CPU. Preliminary measurements at a 100 kHz proving rate show ~20 seconds single-threaded (dominated by building the quotient polynomial from ~131K roots per instance via subproduct tree, optimized with split-commit: K smaller polynomials committed in a smaller FRI domain); CPU parallelism alone could reduce this to ~5-10 seconds, and GPU acceleration — to which NTT is naturally suited — to under one second. Minimizing PCS latency is critical since it executes only at the end of the proving pipeline and cannot be overlapped with earlier stages.

**Protocol Sequencing.** The PCS phase cannot overlap with batching execution: the prover needs the verifier's OT delta (revealed after batching completes) to reconstruct the zero MAC values for all branches before constructing the commitment polynomial.

**Polynomial Commitment Degree Guarantees.** Standard FRI-based PCS constructions enforce an upper bound on polynomial degree, but our setting additionally requires preventing the prover from committing fewer values than expected. To address this, the quotient polynomial Q = P / Z is constrained to have degree exactly 2^k - 1, so the FRI domain size 2^k leaves zero slack. If the prover commits fewer roots (smaller deg(Z)), the resulting quotient exceeds degree 2^k - 1, violating the FRI bound and causing verification to fail. This zero-slack parameterization effectively turns FRI's upper bound into an exact-degree guarantee on Z.

**Zero-Knowledge of the PCS.** FRI-based polynomial commitment does not provide zero-knowledge by default. We achieve hiding by padding Z with ~20 random dummy roots, so the verifier cannot distinguish real committed values from blinding values (C(520,20) ~ 2^130 brute-force security). Plonky3's built-in HidingFriPcs cannot be used here as it would destroy the tight degree bound required by our exact-degree guarantee.

## Optimization 2: Lifting MACs to GF(2^64) instead of GF(2^128)

The Boolean protocol lifts single-bit MACs into MACs over GF(2^128) using a random linear combination (RLC) with random challenge weights. The purpose of this lifting is to compress many individual bit-level checks into a single field-level check: if any bit-MAC is invalid, the lifted MAC is nonzero with overwhelming probability. The security guarantee is that a cheating prover can guess the correct MAC with probability at most 2^{-128}.

However, 128-bit statistical security appears excessive for this application. A 64-bit security level (probability 2^{-64} of a successful forgery) is sufficient in practice. Lifting to GF(2^64) instead of GF(2^128) would reduce the cost of each `gfmul` operation by approximately 3-4x, since GF(2^128) multiplication requires 3-4 `PCLMULQDQ` (carry-less multiply) instructions via Karatsuba decomposition, whereas GF(2^64) requires only a single `PCLMULQDQ` plus a simpler reduction. The performance gain is well-understood (3-4x fewer `PCLMULQDQ` instructions per `gfmul`); what remains to be verified is that reducing the field size does not violate any security claims in the protocol. The change appears sound on its face — the RLC only requires statistical security against MAC forgery — but a careful analysis is needed to confirm that no other part of the protocol relies on the full 128-bit field size.

Benchmarks with realistic zkVM parameters (320 inputs, 300 AND gates, 400 branches, 10K batches) showed that this optimization yields approximately a 50% speedup for the proof generation phase.

An additional benefit: this would likely halve the computational cost of silent OT generation (the Silver protocol). Silver first produces VOLEs over single bits, then packs them into VOLEs over the larger field. Packing into GF(2^64) instead of GF(2^128) requires half as many single-bit VOLEs, reducing the Silver protocol's computation accordingly.

## Optimization 3: Variable-Size Branch Circuits

The original protocol requires all branch circuits to have equal size. This requirement does not stem from correctness, but from control-flow privacy: during OT setup, differing numbers of OTs per branch would allow the verifier to infer which branch the prover selected.

Crucially, this uniformity constraint applies only to the OT communication layer. The OT allocation must be padded to the maximum branch size so that all branches appear identical on the wire. However, from a computational standpoint, branches may have different circuit sizes. Both parties evaluate all branches regardless of which one is active, and the proof for each branch only requires field multiplications proportional to its actual gate count. This decouples computation cost from the maximum branch size: instead of paying for the largest branch on every evaluation, each branch pays only for its own gates.

This observation is particularly impactful in practice, since the majority of CPU instructions are simple. For example, incrementing a program counter requires on the order of 17 AND gates, while comparing an instruction opcode against a constant requires approximately 10 additional gates. Most instructions fit comfortably within a budget of roughly 100 AND gates. More complex operations, such as division, are comparatively rare. A promising direction — not yet implemented — is to represent such operations as sequences of multiple consecutive instructions, each constrained to the same compact gate budget. This would modestly increase the execution trace length (estimated ~10%) but enable a much tighter bound on per-branch computation.

Preliminary benchmarks using a simplified model (95% of branches at 100 AND gates, 5% at full size; 320 inputs, 300 AND gates max, 200 branches, 10K batches) show that proof generation completes in approximately 4 seconds. Extrapolating to realistic zkVM parameters, this suggests the protocol could achieve proving rates approaching 20 kHz while remaining within the 1 MB prover bandwidth constraint. Further work is needed to validate this projection with actual zkVM circuit layouts.

Note: this optimization requires careful review to ensure it does not conflict with the protocol's formal security proof. The original proof assumes uniform branch sizes; allowing variable sizes changes the structure of the prover's computation and the information available to the verifier (even if OT padding hides the branch choice). A formal argument is needed to confirm that the simulation-based proof still holds under this relaxation.

## Empirical Evaluation

To validate the feasibility of the optimized binary protocol under our bandwidth constraints, we conducted an empirical benchmark using a Boolean circuit configuration.

The circuit was instantiated with 320 input gates, corresponding to eight 32-bit registers, the program counter (PC), and the instruction word — reflecting our design choice to use a register-based zkVM rather than a natively stack-based model as required by WASM.

Each branch circuit was constrained to 110 AND gates, and the total number of branches was set to 200. While empirical measurements on a JSON-parsing workload indicate that typical programs require roughly 50 distinct instructions, we conservatively increased this bound to accommodate the splitting of complex instructions — such as division — into multiple simpler steps (see Optimization 3). The proving rate was fixed at 20 kHz.

Under this configuration, end-to-end proof generation completes in approximately two seconds, with prover upload remaining within the 1 MB bandwidth budget. The dominant auxiliary cost is the generation of approximately 8 million correlated OTs. Using a protocol such as SILVER, this is estimated at roughly three to four seconds on a single thread. However, since the benchmark was executed single-threaded, both OT generation and the proving computation itself admit parallelization across additional threads, reducing overall latency to well under one second in practice.

The benchmark can be reproduced with:

```
./run_bool 320 110 200 20000
```

Result (single-threaded, Intel i7):

```
=== Batchman Prover (Boolean) ===
Inputs: 320, ANDs: 110
Branches: 200, Batches: 20000
Total: 1935.24 ms
COTs: 8600000 (8.6M)
  (COTs are pre-generated; not included in runtime)
Bytes sent: 1077376 (1052.12 KB)
Bytes recv: 32 (0.03125 KB)

Verifier: PASSED
```

## Limitations and Open Questions

The optimizations above rely on a binary protocol, which is not directly compatible with existing ZK-RAM constructions. In particular, the "Two Shuffles" protocol — the most efficient known ZK-RAM — operates exclusively over prime fields. As a result, the current approach proves only CPU execution; memory accesses remain unaccounted for.

Achieving a complete zkVM requires proving RAM accesses as well. One potential approach is bit-to-arithmetic (B2A) conversion into a prime field. However, to preserve obliviousness, this conversion must occur at every execution step — including steps with no actual memory access (via dummy accesses). Whether the resulting communication overhead is compatible with the strict bandwidth constraints considered in this work remains an open question and a critical direction for future investigation.

An alternative direction is to reuse an existing lookup argument as employed in STARK-based zkVMs, such as LogUp. This could allow RAM accesses to be verified without requiring a full transition to arithmetic circuits.

The main challenge is reconciling Batchman's IT-MAC semantics — where each bit is secret-shared between the parties using 64-bit IT-MACs (reduced from 128-bit per Optimization 2) — with the algebraic structure expected by LogUp. A key open question is how to faithfully embed these shared representations into the lookup argument without leaking information or incurring prohibitive overhead.

Preliminary investigation suggests this may be achievable by having the prover commit to a cryptographic hash or algebraic aggregation of its IT-MAC shares, then selectively open these commitments in zero knowledge as part of LogUp. However, the soundness, efficiency, and communication costs of this approach remain unclear and require further study.
